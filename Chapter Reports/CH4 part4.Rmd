---
title: "Chapter 4 part4 *Descriptive Statistics* (Special Central Tendencies, Cumulative Distribution Function, Skewness and Kurtosis)"
author: "Nurrospody"
date: "5/5/2020, *Learn R for Applied Statistics : With Data Visualizations, Regressions, and Statistics*"
output:
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Central Tendency, Spread, Variation Commands that DO require manual help CONTINUED

In the last report we ended by talking about getting the mode of a dataset or variable, and how that requires making a special function or frequency tahble.  
Next, we'll talk about getting **POPULATION** (POP) **Standard Deviation and Variance**, which also require 'special' treatment.  

Because of Bessel's correction--which is meant to partially correct bias of standard deviation estimation--we need to append ```* (N - 1)/N:``` to our variance var() function.  N is length(variable), or the amount of points in our variable.  N-1 is used for similar reasons as n-1 in other statistical formulae.   
In order to get the POP Standard Deviation, we do the above the get the POP variance, and then sqrt(variance).  We do this, rather than ```sd(X) * (N-1) / N;```, because Variance is a better unbiased estimator than Standard Deviation.  So in order to get the POP Standard Deviation in R, you must get the POP Variance first.

Let's calculate the POP Variance and Standard deviation using our 'random' variable.
```{r random setup, include=FALSE}
random <- rnorm(70, 10, 2);
```
```{r ran}
N <- length(random);
variance <- var(random) * (N - 1) / N;
N; variance; sqrt(variance);
```
The first number in the solution box is N (70), the second number is the POP Variance (probably about 4), and the third number is the POP Standard Deviation (probably about 2).  Because Standard Deviation is the square of variance, it of course has a less wide range than variance does.  I guess that inherently makes it more biased because it is smaller and more concentrated.  

#### Cumulative Distribution Function (CDF) **New Commands: pnorm(), qnorm()**  
I hadn't learned about CDF before, or at least I hadn't learned it by name, so I looked up examples of what this was before I tried using the function.  
CDF uses 4 components: probability, mean, standard deviation, an X value.  
Given a certain X value, the mean, and standard deviation, CDF can tell you how PROBABLE it is that X is (or is less than) that specific value.  (If you want to know how probable it is that X is GREATER than or less than a certain value, you need to write it into the equation; kind of like how getting area right of a curve in a statistics textbook requires you to subtract your result from 1).  So if you have a 6-sided dice and roll it, it is 1/6 probable that you will get a result of 1 or lower.  
This is the idea behind the pnorm() function.  

You can also use a given probability, mean, and standard deviation, to solve for a specific X value.  In this case, the given probability is like the area of a curve, so 0.95 probabilty has only 0.05 area to the right of the curve.  If you use CDF you can ask R to solve what X value is sitting right on that probability/area.  
This is the idea behind the qnorm() function.  

I noted the interesting naming scheme between these functions--q and p in confidence intervals.  q represents the recorded probability (so the recorded frequency) that something was FALSE, and p is that something was TRUE.  q is often simply shown as 1 - p.  I thought it was interesting that both of these had to do with probability.  

Let's practice using the pnorm() and qnorm() functions.  I'll use the same mean (10) and standard deviation (2) as in 'random', 
```{r pnorm}
#Here I'm checking to see how likely a result of 16 is.  Sometimes knitting has given me one or two of these extreme values.  
pnorm(16, 10, 2);
```
```{r qnorm}
#Here I'm asking what the 50 percentile is.  Hopefully, it returns the mean.
#I also asked for a different value, about 1 standard deviation away from the mean.
qnorm(0.50, 10, 2); mean(random); qnorm(0.84, 10, 2);
```
The answer I got from pnorm() obviously needs to be subtracted from 1 to get the real probability.  This is because pnorm() returns the probability something is less than or equal to a value, but I want the probability that it's greater than or equal to.  We can do that with:
```{r one minus qnorm}
1 - pnorm(16, 10, 2);
```

#### Next is the section on skewness and kurtosis.  **New Commands: r, getOption, options, "moments", skewness(), kurtosis(), as.data.frame(), plot(), polygon(), density()**
I'm familiar with skewness as a vague concept, but I've never seen it used as a calculation or number before.  I had never heard of kurtosis at all.  I [referred to a math help website](https://brownmath.com/stat/shape.htm) to give me a basic overview of what these concepts are.  
**Skew** Data can be *approximately symmetric*, *moderately skewed*, or *highly skewed*.  
0 is perfectly symmetric, but this is unlikely for real-world data.  
Skewness scores ranging from + 0.5 to - 0.5 are approximately symmetric.
Skewness scores of +1 to -1, non-inclusive of the 0.5 range, are moderately skewed.  
Anything more extreme is highly skewed.  
Some data *looks* skewed to one side, but can be treated as approximately normal anyway because of this.  I never knew this bit of information, which is rather helpful because it always felt like we arbitrarily decided if something was "close enough" before.

To test for skew (and kurtosis) in R we must install and require the "moments" package.  I got a strange 'trying to use CRAN without setting a mirror' error, so I had to re-set the CRAN repository for my knit session; I think something didn't get shut down correctly when I last worked.
```{r mom}
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)
install.packages("moments")
require(moments)
hist(random); skewness(random)
```
Unless something terribly extreme happened with the normal distribution random seed, the skewness result should be approximately normal and a very small number.  But what about kurtosis?  That can vary more.  

**Kurtosis** measures the slope's *shape*--how much variance comes from infrequent extreme deviations, rather than frequent & modest deviations?  Higher kurtosis will have longer tails (and thus a higher peak) while lower kurtosis will have less tails (and thus a rounder peak).  
You can have 3 graphs that all have the same mean, same stdev, and same skewness, but look very different because the kurtosis.  This works out mathematically because intermediate values become rarer and extreme values become more likely.  
A perfectly normal cuve has a kurtosis of 3.  Because of this, we typically work with "excess" kurtosis (kurtosis - 3) so that our perfect normal distribution is actually 0.  The lowest kurtosis possible (two discrete probabilities--flipping a coin) is 1 (or excess -2) and Student's t with 4 df has infinity kurtosis.  
```{r}
kurtosis(random);
```
R appears to use "excess" sample kurtosis as its returned value, like Excel does.  
Kurtosis is easier to visualize with a plot than a histogram, but I didn't want to use a Q-Q plot for it because that just shows normality, not the actual curve shape (which is what's unique here with kurtosis).  I found Kernel Density Plots as a solution to show it off for now.  The density argument needed to be numeric, and not the name of a variable, so I used as.data.frame(random) to set it.  A list of 70 numbers is quite long so I used include=FALSE for that particular line of code.
```{r, include=FALSE}
as.data.frame(random)
```
```{r}
d <- density(random)
plot(d, main="Distribution of the 'random' normal distributed variable")
polygon(d, col="red", border="blue") 
```





To continue reading the CH4 reports, select a new section:  
[Part 5 of the Chapter 4 Reports](https://github.com/Nurrospody/SOURCE-Statistics-ILC/blob/master/Chapter%20Reports/CH4-part5.md)  
[Link to README to select any Chapter Report](https://github.com/Nurrospody/SOURCE-Statistics-ILC/blob/master/README.md)
