---
title: "CH6 part 3 *Inferential Statistics & Regressions*"
author: "Nurrospody"
date: "8/7/2020, *Learn R for Applied Statistics : With Data Visualizations, Regressions, and Statistics*"
output:
  github_document:
    toc: TRUE
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

Modes <- function(x) {
  ux <- unique(x)
  tab <- tabulate(match(x, ux))
  ux[tab == max(tab)]
}

Madoka <- read.csv(file="../data_sources/Madoka Magika Volume 1-3.csv")

Smallmad <- Madoka[1:5,]
require(dplyr)

Crop <- read.csv(file="../data_sources/crop.data.csv")
require(AICcmodavg)
require(ggplot2)

require(xlsx)
Garden <- read.xlsx(file="../data_sources/Garden Practicum.xlsx", 1)
Garden2 <- read.xlsx(file="../data_sources/Garden Practicum.xlsx", 2)
require(moments)
FB <- read.xlsx(file="../data_sources/FriendshipBonus.xlsx", 1)
SleepyFB <- read.xlsx(file="../data_sources/PCM_Pokemon_Visitors_Data_Data.xlsx", 2)
library(random)
```

### MANOVA  
*While an ANOVA tests for 'what is the effect on X based on 3+ different variables', MANOVA tests for 'what is the effect of x variable on 3 different outcomes'.*  MANOVA is *multi*variate analysis of variance, rather than a (single) variate analysis of variance.  It seems to be less common, as there is less information about MANOVA than ANOVA.  Even the ```help(manova)``` page doesn't have any commands and simply redirects to the ```help(aov)``` page.

Despite the similar names, they're used for very different things.  I wouldn't be surprised if using a MANOVA on a dataset where it's not warranted, would result in 'fake results' that could be exploited for publicity . . .  because of this I don't want to try and contort my new n134 dataset about friendship data to fit into an ANOVA.  My dataset that I collected, doesn't really have enough factors either, because I didn't collect anything about the weather or bugs or other extraneous things.  I'll use the iris data instead.  

#### Iris dataset summary  

```{r}
data("iris")
str(iris)
summary(iris)
```

With this dataset we have one variable--species--that we can clearly hypothesize about having an effect on the other 4 variables (sepal and petal width/length).  MANOVA doesn't appear to use any special 'does this variable effect the other variables' like ANOVA does, because that's not the point here.  We just ask to concatenate bind some variables together, and ~ it to another variable while specifying our data.  

#### MANOVA with all 4 dependent variables  
```{r}
MrIris <- manova(cbind(iris$Sepal.Length, iris$Sepal.Width, iris$Petal.Length, iris$Petal.Width) ~ iris$Species, data=iris);
summary(MrIris, intercept = TRUE)
```
What we can see here is that, based on coorelation between these 4 variables, they were definitely impacted by the species of flower.  But how to tell which variable was impacted by species?  I can use the summary.aov() command for that, and it'll spit out different Responses (1, 2, 3, 4).  The numbers of the responses co-incide which what order the variables were put into the MrIris variable.  ~~he's Mister Iris because he's a MANova~~  

#### Follow-up analysis
```{r}
summary.aov(MrIris)
```
We can see that the p-value of all these variables is *actually* the same.  That makes sense, because a flower's species has just as much impact on its sepals, as its petals, as everything else.  In a dataset that wasn't asking something basic like, how does what something is impact how it is, or how much does the circumfrence and radius of a circle coorelate (essentially), we would get differing p-values and that would narrow down our focus.  Othertimes, we might even get a MANOVA that proves there is a significant difference between one group and another, but the univariate ANOVA follow-up doesn't accomodate for variable coorelation.  This seems to impact whether you can do a 'contrast' or not with intercepts.  

I found a source that discussed using 'contrast models' after doing an ANOVA, but we don't really have any data to contrast here.  There are notes about these contrasts on github [here](https://gaopinghuang0.github.io/2017/11/20/MANOVA-notes-and-R-code).   

### Non-parametric: Wilcoxon Signed Rank Test (efficiency 0.95)  
We can use this test for testing if *matched pairs* have some property where the median is equal (or not) to zero  
or for testing a claim that the population has a median equal to a claimed value (single-t test).  

We need to pair each sample with a claimed median (even if that's just the null hypothesis being paired with everything) so that we have matched pairs, due to the nature of this test.  
We need a simple random sample and approximately symmetric distribution; but we do not need a normal distribution.  Use this for t-tests that do not have a normal distribution, essentially.  It's not really important that the distrubution is NOT normal, but if it IS normal, you're wasting data by using this test rather than a t-test.  
I've made the mistake of not checking my datasets for normality before running them through t-tests, but I suppose I did not know how to do non-parametrics in R beforehand anyway and it had been a little while since I thought about it.  

#### Testing for (lack of) normality and presence of symmetry  

For my test I used my friendship bonus $stagessince data.   
This is because I can't prove that the data is normally distributed.  If data is normally distributed, the p-value with a shapiro-wilk test should be above 0.05, but here it is not--probably because n is only 30.  

```{r}
shapiro.test(FB$StagesSince[2:31])
```

However, I can verify with a skewness test that the data is approximately normally skewed.  (It's between -+0.5).  It is close to being moderately skewed towards smaller numbers, but not significantly enough that we can't use a wilcoxon test.  

```{r}
skewness(FB$StagesSince[2:31])
```

#### Doing the test with just X

I'm going to do a "1-way t-test" replacement to check if the median (not the mean, since this is non-parametric) is equal to 8.  I know that the mean is 8, so I wanted to see what happens.  I will also test for 7 and 9 to see what happens if I 'miss' the mean in my paired data test.  

```{r}
wilcox.test(FB$StagesSince, mu=8, alternative = "two.sided", conf.int = TRUE)
wilcox.test(FB$StagesSince, mu=7, alternative = "two.sided", conf.int = TRUE)
wilcox.test(FB$StagesSince, mu=9, alternative = "two.sided", conf.int = TRUE)
```

Each test tells us what the alternate hypothesis is, so that we know what to do with the p-value.  For 8, the probability that the median is equal to 8 is greater than 0.05; but for 7 and 9, this is not the case.  Because of the p-value we can interpret that 8 is the true location of the median, and that 9 and 7 are not the true locations.  
```two.sided``` refers to the fact that we are testing both sides of the probability curve, because it makes equal sense that the alternate hypothesis could result in a number above and below the null hypothesis.  we could also use ```less``` or ```greater``` if they were applicable for the context.  

#### Doing the test wtih X and Y  

First, let's generate X and Y.  I'll be using the random library to make random numbers.  In one version, PAIRED with be TRUE; in the other, FALSE; to see what happens.

```{r}
var1 <- randomNumbers(n=70, min=1, max=500, col=2);
var2 <- randomNumbers(n=70, min=1, max=500, col=2);
wilcox.test(var1, var2, mu=250, alternatives="two.sided", paired=TRUE, conf.int = TRUE)
```
When both X and Y are given and PAIRED is FALSE, it's for comparing two indendent populations rather than dependent populations, which makes it instead a:  

#### Wilcoxon Rank SUM test  


```{r}
wilcox.test(var1, var2, mu=250, alternatives="two.sided", paired=FALSE, conf.int = TRUE)
```
When variables are paired (correctly), coorelations are easier to find and the confidence interval will be more narrow.  Unpaired data should not be falsely paired, however.  

### Wilcoxon-Mann-Whitney Test  
This is the non-parametric equivilant of a 2-way t-test, for comparing the medians between two different variables.  We can test for if there's a significant difference beween first variable median and second variable median.  
Let's use friendship bonus data; both the data I received from LordArcanite#1325 ```(FB$StagesSince[2:31])``` and from SleepyRider#7340```SleepyFB$FBGap[1:19]```.  Sleepy's data collection was comphrehensive for general stage information, but in this case I will be comparing only the StagesSince variable from Arc's data and the FBGap variable from Sleepy's data (bolded only), of which there is n19.  Because n<30 I am using a non-parametric test for the comparison of medians for these two data sets.

```{r}
wilcox.test(FB$StagesSince[2:31], SleepyFB$FBGap[1:19], correct = FALSE, conf.int = TRUE)
```
Here, we cannot disprove the null hypothesis that the true location shift of the median is 0. We also see that the confidence variable includes 0, meaning that we cannot be confident that one median is either below or above the other.  However, for this data, this is a good thing!  What this means is that the median is approximately the same for both sets of data.  Of course, we must take this with a grain of salt, because n is only 19 for one of these--but it is the expected result.  

To continue reading the CH6 reports, select a new section:  
Next: [Part 4 of the Chapter 6 Reports](https://github.com/Nurrospody/SOURCE-Statistics-ILC/blob/master/Chapter%20Reports/CH6-part4.md)  
Previous: [Part 2 of the Chapter 6 Reports](https://github.com/Nurrospody/SOURCE-Statistics-ILC/blob/master/Chapter%20Reports/CH6-part2.md)   
  
[Link to README to select any Chapter Report](https://github.com/Nurrospody/SOURCE-Statistics-ILC/blob/master/README.md)
