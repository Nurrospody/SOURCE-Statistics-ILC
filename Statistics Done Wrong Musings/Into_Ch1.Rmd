---
title: "Statistics Done Wrong Intro & Ch1 Musings"
author: "Nurrospody"
date: "4/19/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
This isn’t a literature class with assigned reading and I don’t expect my sponsor to read the same books I do, but something I’ve really wanted to do is read a book I’ve had for a while: Statistics Done Wrong by Alex Reinhart.  

Engaging with the book was useful because it’s impossible to know what you do not know, and while I’m familiar with p-values, hypothesis testing, and confidence intervals . . . I’m not familiar with proportional hazards models, propensity scores, or least-square fits, and I didn’t know that so little of the scientific community has actual knowledge of how to use statistical tests.  
  
  -
  
Something Wikipedia doesn’t allow for is ‘independent research’–like, you can’t read a scientific paper, understand that the claim they’re making with their unsupporting numbers is actually bogus, and then mention that the findings are bogus in the article. All you can do is supply what the paper concludes. I encountered multiple papers about animal behaviour in my Anthrzoology class that seemed to make bogus claims with unproperly supported data (claiming that female dogs complete the human eye-contact oxytocin bonding loop, even though the data was inconclusive at best and only when the dogs were drugged; or claiming that rats felt empathy by releasing trapped cagemates, even though there was no data measured about the trapped rats whatsoever and the researchers never actually tested for distress in the first place).  
The into to this book claims that perhaps more than half of scientific papers make common statistical errors; statisticians don’t tend to be employed to review papers; some scientists even conclude that maybe this means most published findings are probably false. Statistical training is very poor and not required for many professions that really ought to have it, so that professionals can understand more than a fifth of the studies out there for their field.  I mention Wikipedia and those 2 animal articles because reading this content in my book makes me feel less alone in my occasional disgruntledness with papers.  Maybe I'm not disgruntled enough.  
  
  -

The rarity of statistics knowledge makes me, as someone who wants to pursue statistics, feel powerful and in demand, I guess.  

Data torture–using multiple “proper” statistical tests until one gives a result that you want–was mentioned, as is the fact that ‘unexciting’ papers that claim “no effect” (or the opposite effect that you want) don’t get submitted OR they don’t get published if they are submitted. I think making ‘no effect’ results public are really important; that’s part of why I try to discuss things that I tinker with but didn’t get any fruit out of yet, like when I tried to make my own data.frame. I also think we ought to know–if something was ran through 9 tests and only 2 of them gave the result we want . . . well, 7 is more than 2, and the 2 shouldn’t be what we report.  
  
  -

Chapter 1 discussed p-values which wasn’t new information for me, but I was thrilled to know that my understanding of what a p-value is, is correct (given that apparently, it’s often misunderstood): the probability that a given result could have happened if random chance and luck alone was responsible. But the book also mentioned that if you have a great number of data points you can try to pigeonhole any insignificant, desired value as ‘less than 5% it happened by chance alone’ by sheer quantity alone, and that seems to be where reporting confidence values instead of regurgitating a p-value can be handy. CIs can give a lot more information that p-values, and if they’re embarrassingly wide, it can be a hint that the sample size isn’t large enough. (Unfortunately, it can also be a hint that it’s a confidence interval, since they’re often kind of embarrassing.)  

The book mentions that a p-value of 0.032–3.2% that something happened by chance alone–is not equivalent to “my false positive rate is 3.2%”. False positive rates are based on experiment design, and the power of a hypothesis test. (Power wasn’t mentioned, but I remember it briefly, and it’s an interesting concept).  
  
  -

When I read through the intro and chapter 1, I printed them out so I could highlight them and write on the papers. It’s a satisfying way to engage with the text but it also makes my reading embarrassingly slow: about 4 minutes per page! It’s a nice activity to do when I need a toned-down day but still want to do work.
